
release-date: 2024-05-30
arxiv: https://arxiv.org/abs/2405.20092v1
github: x

[Divide-and-Conquer 전략]
1. 문제 분해 (Divide)
- 복잡한 코딩 문제를 더 작고 관리하기 쉬운 하위 함수들로 분해
- 모델이 동적으로 새로운 함수를 도입하며 문제를 점진적으로 해결
- 재귀적 접근: 각 하위 함수도 필요시 더 작은 함수로 분해 가능
- 깊이 우선 탐색(DFS)으로 함수 트리 구성, 최대 깊이는 6으로 제한
- 구현 세부사항:
  - 온도(temperature) 0.2로 Python 코드 블록 생성
  - 순환 참조 방지를 위해 새 하위 함수가 기존 함수를 참조하지 않도록 제한
  - 유효한 함수 시그니처를 포함한 코드를 찾을 때까지 최대 3회 시도

2. 문제 해결 (Conquer)
- 하위 함수들을 조합하여 더 복잡한 목표 달성
- 역방향 위상 정렬 순서로 함수 재생성: 리프 노드부터 시작해 루트까지
- 하위 함수 구현을 활용하여 부모 함수 재작성
- 구현 세부사항:
  - 온도 0.8로 다중 구현 샘플링
 - 기능적 합의(Functional Consensus)를 통해 최적의 구현 선택

[기능적 합의 (Functional Consensus)]
- 목적: 프로그램 동작의 일관성 확보 및 오류 전파 완화
- 방법: 여러 함수 구현을 샘플링하고, 동작 유사성이 가장 높은 것 선택
- 구현:
  1. Consensus@k: k-1개 새 구현 + 1개 기존 구현 = k개 후보 프로그램
  2. 잠재적 입력 X 생성 (프롬프트 사용)
  3. 각 입력에 대해 후보 프로그램 실행
  4. 동일 출력 시 유사성 점수 증가
  5. 예외 발생 또는 시간 초과 시 -100점 부여
  6. 최고 점수 구현 선택

성능:
- HumanEval에서 GPT-3.5 사용 시 94.7% 상한 성능(Pass@11) 대비 Pass@1에서 90.2% 달성
- 자체 테스트 방식보다 우수한 성능

[데이터셋 및 평가]
1. 코드 생성 벤치마크
- HumanEval: 164개 전체 테스트 세트 사용
- MBPP: 200개 샘플 사용
- xCodeEval: 500개 샘플 사용, 난이도별 분류 (Easy, Mid, Hard, Expert)
- 평가 지표: Pass@1 (모든 시스템 테스트 통과 필요)

2. 수학 추론 벤치마크
- MATH: 500개 무작위 샘플 사용 (7개 주제, 5개 난이도 레벨)
- 평가 지표: EM-GPT (GPT-4로 일관성 평가)

[모델 및 구현 상세]
- 사용 모델: 
  GPT-3.5 (gpt-3.5-turbo-0613)
  GPT-4 (gpt-4-1106-preview)
  Llama3 (Meta-Llama-3-8B-Instruct)
  StableCode (stable-code-instruct-3b)
  CodeLlama (CodeLlama-34b-Instruct-hf)
- 추론 환경: vLLM 사용, A100-80G GPU, BF16 정밀도

[FUNCODER 학습 방법]
1. Divide 단계
- 2-shot 프롬프트 사용
- 새 함수 도입 예시와 단순 구현 예시 포함

2. Conquer 단계
- 1-shot 데모로 현재 함수 재작성 요청
- 기능적 합의 적용

3. 기능적 합의
- 1-shot 프롬프트로 잠재적 입력 생성
- 루트 노드의 예제 입/출력으로 잘못된 기능 필터링

[성능 개선 전략 및 결과]
1. 코드 생성 성능
- GPT-4 사용 시 HumanEval에서 SOTA 대비 +3.3% 향상
- GPT-3.5 사용 시 xCodeEval에서 35.3% 성능 향상
- 오픈소스 모델 성능:
  Llama3(8B), StableCode(3B)로 GPT-3.5의 118% 성능 달성
  HumanEval에서 GPT-4의 97% 수준 도달

2. 수학 추론 성능
- GPT-4 사용 시 MATH에서 SOTA 대비 6.0 향상
- GPT-3.5 사용 시 PoT 대비 31.7% 향상
- 오픈소스 모델 성능:
  Llama3로 PoT 대비 38.0% 향상
  StableCode, CodeLlama로 각각 84.7%, 60.5% 향상

[Ablation Study 및 토큰 사용량]
- HumanEval, GPT-3.5 기준
1. Standard: 68.3 (886.7 토큰)
2. One-pass: 72.6 (+4.3) (1233.7 토큰)
3. Two-pass: 78.7 (+10.4) (3343.2 토큰)
4. Two-pass + ST@11: 80.5 (+12.2) (5408.3 토큰)
5. FUNCODER@5: 83.5 (+15.2) (4040.8 토큰)
6. FUNCODER@11: 85.4 (+17.1) (5402.0 토큰)

- LDB (이전 SOTA) 대비:
  성능 2.5 향상, 토큰 사용량 76.5% 감소

[주요 시사점 및 엔지니어링 팁]
1. 동적 함수 분해의 효과
- 복잡한 요구사항 처리에 효과적
- 코드 생성 시 인간의 프로그래밍 방식과 유사한 접근 가능

2. 기능적 합의의 장점
- 자체 테스트보다 정확도 평가에 우수
- 구현 간 미묘한 차이 포착 가능

3. 모델 크기와 성능
- 작은 모델도 효과적인 전략으로 큰 모델에 근접한 성능 달성 가능
- Llama3(8B), StableCode(3B)로 GPT-3.5, GPT-4에 근접한 성능

4. 다단계 접근의 효과
- Divide-and-Conquer와 Consensus 결합으로 누적 성능 향상
- 복잡한 코드 생성 작업에서 특히 효과적

5. 토큰 사용량 최적화
- FUNCODER 방식으로 높은 성능과 함께 토큰 사용량 감소 가능

6. 다양한 태스크에 대한 일반화
- 코드 생성뿐만 아니라 수학적 추론 등 다양한 분야에 적용 가능

7. 오픈소스 모델 활용
- 상업용 대형 모델 없이도 고성능 달성 가능
- 특화된 전략으로 모델 크기의 한계 극복 가능
