Hover over this sentence to see the translation. [ë²ˆì—­](# "ì´ ë¬¸ì¥ ìœ„ì— ë§ˆìš°ìŠ¤ë¥¼ ì˜¬ë¦¬ë©´ ë²ˆì—­ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
Click here to see the translation. [í•œê¸€ í•´ì„ ë³´ê¸°](https://example.com/translation)


ğŸ’¡ Tip
> It introduces phi-1, a model with 1.3B parameters that obtains a pass@1 rate of 50.6% on HumanEval thanks to a novel training process. Unfortunately, the weights are not available.  
ìƒˆë¡œìš´ í›ˆë ¨ í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ 13ì–µ ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§„ ëª¨ë¸ì¸ phi-1ì„ ë„ì…í•˜ì—¬ HumanEvalì—ì„œ 50.6%ì˜ í•©ê²©ë¥ ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì•„ì‰½ê²Œë„ ê°€ì¤‘ì¹˜ëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.


ğŸ“ Paper: https://arxiv.org/pdf/2306.11644.pdf

The authors argue that high quality data can change the shape of the scaling laws, allowing small models to match the performance of bigger ones.  
ì €ìëŠ” ê³ í’ˆì§ˆ ë°ì´í„°ê°€ ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì˜ í˜•íƒœë¥¼ ë³€í™”ì‹œì¼œ ì‘ì€ ëª¨ë¸ë„ í° ëª¨ë¸ì˜ ì„±ëŠ¥ì— ë§ì¶œ ìˆ˜ ìˆë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤.
![image](https://github.com/SonWY2/paper_caputred_images_repo/assets/36894403/3bc8b10b-13bc-4f40-96f2-c5a4c014e37c)



# The importance of high-quality data


**Motivation**: The authors observe that standard code datasets like The Stack, StackOverflow and CodeContests suffer from several drawbacks: samples are not self-contained but referenced, a lot of them are trivial while the most complex ones are poorly documented, and the overall distribution is skewed towards certain topics and use cases.  
**Motivation**: ì €ìë“¤ì€ The Stack, StackOverflow, CodeContestsì™€ ê°™ì€ í‘œì¤€ ì½”ë“œ ë°ì´í„° ì„¸íŠ¸ê°€ ëª‡ ê°€ì§€ ë‹¨ì ì„ ê°€ì§€ê³  ìˆë‹¤ê³  ì§€ì í•©ë‹ˆë‹¤. ìƒ˜í”Œì´ ë…ë¦½ì ì´ì§€ ì•Šê³  ì°¸ì¡°ë˜ê³ , ë§ì€ ìƒ˜í”Œì´ ì‚¬ì†Œí•œ ë°˜ë©´ ê°€ì¥ ë³µì¡í•œ ìƒ˜í”Œì€ ì œëŒ€ë¡œ ë¬¸ì„œí™”ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©°, ì „ë°˜ì ì¸ ë¶„í¬ê°€ íŠ¹ì • ì£¼ì œì™€ ì‚¬ìš© ì‚¬ë¡€ì— ì¹˜ìš°ì³ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤.


They train their solution (phi-1) on a new dataset (<7B tokens) as follows:  
ë‹¤ìŒê³¼ ê°™ì´ ìƒˆë¡œìš´ ë°ì´í„° ì„¸íŠ¸(7B í† í° ë¯¸ë§Œ)ì— ëŒ€í•´ ì†”ë£¨ì…˜(phi-1)ì„ í›ˆë ¨í•©ë‹ˆë‹¤:


**Pretraining** on CodeTextbook, comprised of a filtered version of The Stack and StackOverflow(~6B tokens) + a synthetic samples generated by GPT-3.5 (<1B tokens)  
**Fine-tuning** on CodeExercises, a small synthetic dataset of Python exercises and solutions also generated by GPT-3.5 (~180M tokens)  
í•„í„°ë§ëœ ë²„ì „ì˜ ë” ìŠ¤íƒ ë° ìŠ¤íƒì˜¤ë²„í”Œë¡œìš°(~60ì–µ í† í°) + GPT-3.5ì—ì„œ ìƒì„±í•œ í•©ì„± ìƒ˜í”Œ(<10ì–µ í† í°)ë¡œ êµ¬ì„±ëœ CodeTextbookì— ëŒ€í•œ **ì‚¬ì „ êµìœ¡**.  
GPT-3.5(~1ì–µ 8ì²œë§Œ í† í°)ì—ì„œ ìƒì„±ëœ íŒŒì´ì¬ ì—°ìŠµ ë° ì†”ë£¨ì…˜ì˜ ì†Œê·œëª¨ í•©ì„± ë°ì´í„° ì„¸íŠ¸ì¸ CodeExercisesì˜ **íŒŒì¸ íŠœë‹**.




# Filtering code
The authors use the Python subset of the deduplicated version of The Stack and StackOverflow (35B tokens). They used **GPT-4** to annotate the quality of a subset (100k samples), using the following prompt: â€œdetermine its educational value for a student whose goal is to learn basic coding concepts.â€  
ì €ìë“¤ì€ ì¤‘ë³µ ì œê±°ëœ ë²„ì „ì˜ ë” ìŠ¤íƒê³¼ ìŠ¤íƒì˜¤ë²„í”Œë¡œìš°(35B í† í°)ì˜ íŒŒì´ì¬ í•˜ìœ„ ì§‘í•©ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ì´ë“¤ì€ **GPT-4**ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í•˜ìœ„ ì§‘í•©(10ë§Œ ê°œì˜ ìƒ˜í”Œ)ì˜ í’ˆì§ˆì— ì£¼ì„ì„ ë‹¬ì•˜ìŠµë‹ˆë‹¤: "ê¸°ë³¸ ì½”ë”© ê°œë…ì„ ë°°ìš°ëŠ” ê²ƒì´ ëª©í‘œì¸ í•™ìƒì„ ìœ„í•œ êµìœ¡ì  ê°€ì¹˜ë¥¼ ê²°ì •í•˜ì„¸ìš”."


ğŸ’¡ Tip
> This prompt could probably be improved by asking GPT-4 to break down its reasoning into steps before outputting the final value.  
> ì´ í”„ë¡¬í”„íŠ¸ëŠ” ìµœì¢… ê°’ì„ ì¶œë ¥í•˜ê¸° ì „ì— GPT-4ì— ì¶”ë¡ ì„ ì—¬ëŸ¬ ë‹¨ê³„ë¡œ ì„¸ë¶„í™”í•˜ë„ë¡ ìš”ì²­í•˜ì—¬ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

This creates a dataset of code snippets and corresponding values. The authors produce embeddings of each code snippet using a pretrained CodeGen model, and train a random forest classifier to predict the quality of each sample.  
ì´ë ‡ê²Œ í•˜ë©´ ì½”ë“œ ìŠ¤ë‹ˆí«ê³¼ í•´ë‹¹ ê°’ì˜ ë°ì´í„° ì„¸íŠ¸ê°€ ìƒì„±ë©ë‹ˆë‹¤. ì‘ì„±ìëŠ” ì‚¬ì „ í•™ìŠµëœ CodeGen ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê° ì½”ë“œ ì¡°ê°ì˜ ì„ë² ë”©ì„ ìƒì„±í•˜ê³  ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜ê¸°ë¥¼ í•™ìŠµí•˜ì—¬ ê° ìƒ˜í”Œì˜ í’ˆì§ˆì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

# Generating synthetic data
The authors argue the synthetic samples should be diverse (concepts, skills, scenarios, difficulty, complexity, style) and non-repetitive to reduce the risk of overfitting/memorizing and be more robust. Inspired by TinyStories, they use randomized seeds Ã  la Alpaca to generate samples with GPT-3.5:  
ì €ìëŠ” í•©ì„± ìƒ˜í”Œì´ ë‹¤ì–‘í•´ì•¼ í•˜ë©°(ê°œë…, ê¸°ìˆ , ì‹œë‚˜ë¦¬ì˜¤, ë‚œì´ë„, ë³µì¡ì„±, ìŠ¤íƒ€ì¼), ë°˜ë³µì ì´ì§€ ì•Šì•„ì•¼ ê³¼ì í•©/ì•”ê¸° ìœ„í—˜ì„ ì¤„ì´ê³  ë” ê°•ë ¥í•˜ë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤. TinyStoriesì—ì„œ ì˜ê°ì„ ë°›ì•„ ì•ŒíŒŒì¹´ì˜ ë¬´ì‘ìœ„ ì‹œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ GPT-3.5ë¡œ ìƒ˜í”Œì„ ìƒì„±í•©ë‹ˆë‹¤:


Synthetic training data (<1B tokens): code and text with examples and constraints.  
CodeExercises (~180M tokens): Python exercises and solutions, where each exercise is a docstring of a function that needs to be completed.  
í•©ì„± í›ˆë ¨ ë°ì´í„°(<1B í† í°): ì˜ˆì œì™€ ì œì•½ ì¡°ê±´ì´ í¬í•¨ëœ ì½”ë“œì™€ í…ìŠ¤íŠ¸.  
ì½”ë“œ ì—°ìŠµ(~1ì–µ 8ì²œë§Œ í† í°): Python ì—°ìŠµ ë¬¸ì œ ë° ì†”ë£¨ì…˜ìœ¼ë¡œ, ê° ì—°ìŠµ ë¬¸ì œëŠ” ì™„ë£Œí•´ì•¼ í•˜ëŠ” í•¨ìˆ˜ì˜ ë¬¸ì„œ ë¬¸ìì—´ì…ë‹ˆë‹¤.

# Model architecture and training
phi-1 is a decoder-only transformer using rotary position embedding, FlashAttention and multi-head attention (MHA), with parallel MHA and MLP layers + codegen-350M-monoâ€™s tokenizer.  
phi-1ì€ íšŒì „ ìœ„ì¹˜ ì„ë² ë”©, í”Œë˜ì‹œì–´í…ì…˜, ë©€í‹°í—¤ë“œ ì–´í…ì…˜(MHA)ì„ ì‚¬ìš©í•˜ëŠ” ë””ì½”ë” ì „ìš© íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ ë³‘ë ¬ MHA ë° MLP ë ˆì´ì–´ + ì½”ë“œì  -350M-monoì˜ í† í°ë¼ì´ì €ê°€ íƒ‘ì¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

ğŸ’¡Tip
> Its architecture is very much inspired by CodeGen and does not include Fill-In-the-Middle or Multi-Query-Attention like StarCoder (low-hanging fruit improvement).  
>ì˜ ì•„í‚¤í…ì²˜ëŠ” CodeGenì—ì„œ ë§¤ìš° ë§ì€ ì˜ê°ì„ ë°›ì•˜ìœ¼ë©° StarCoderì™€ ê°™ì€ ì¤‘ê°„ ì±„ìš°ê¸° ë˜ëŠ” ë‹¤ì¤‘ ì¿¼ë¦¬ ì£¼ì˜(ë‚®ì€ ë§¤ë‹¬ë¦° ì—´ë§¤ ê°œì„ )ë¥¼ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.


Hyperparameters for phi-1 with 1.3B/350M parameters:

24/20 layers  
Hidden dimension = 2048/1024  
MLP-inner dimension = 8192/4096  
32/16 attention heads with dimension = 64/32  
Sequence length = 2048  
Objective = next-token prediction


# The importance of fine-tuning
![image](https://github.com/SonWY2/paper_caputred_images_repo/assets/36894403/05e4d8c3-26b6-44ee-9a42-fac4d0646462)


Fine-tuning phi-1 on CodeExercises greatly improves the modelâ€™s performance, even for tasks that are not in the fine-tuning dataset.  
CodeExercisesì—ì„œ phi-1ì„ ë¯¸ì„¸ ì¡°ì •í•˜ë©´ ë¯¸ì„¸ ì¡°ì • ë°ì´í„° ì„¸íŠ¸ì— ì—†ëŠ” ì‘ì—…ì˜ ê²½ìš°ì—ë„ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤.

The authors notice that the model gets better at interpreting questions and logical relationships in the prompts. Interestingly, it also becomes better at using external libraries, even when they do not appear in the fine-tuning set (e.g., Pygame and Tkinter).  
ì €ìë“¤ì€ ì´ ëª¨ë¸ì´ í”„ë¡¬í”„íŠ¸ì˜ ì§ˆë¬¸ê³¼ ë…¼ë¦¬ì  ê´€ê³„ë¥¼ í•´ì„í•˜ëŠ” ë° ë” ëŠ¥ìˆ™í•´ì§„ë‹¤ëŠ” ì‚¬ì‹¤ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. í¥ë¯¸ë¡­ê²Œë„ ì´ ëª¨ë¸ì€ ë¯¸ì„¸ ì¡°ì • ì„¸íŠ¸ì— í¬í•¨ë˜ì§€ ì•Šì€ ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬(ì˜ˆ: Pygame ë° Tkinter)ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°ë„ ë” ëŠ¥ìˆ™í•´ì¡ŒìŠµë‹ˆë‹¤.


# Performance evaluation
The authors argue HumanEvalâ€™s binary score (code passes the unit tests, or it fails) does not capture the nuances of the modelâ€™s performance. They introduce an LLM grading using GPT-4 (between 0 and 10), which does not require tests.  
ì €ìë“¤ì€ HumanEvalì˜ ì´ì§„ ì ìˆ˜(ì½”ë“œê°€ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ë¥¼ í†µê³¼í•˜ë©´ í•©ê²©, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë¶ˆí•©ê²©)ê°€ ëª¨ë¸ ì„±ëŠ¥ì˜ ë¯¸ë¬˜í•œ ì°¨ì´ë¥¼ í¬ì°©í•˜ì§€ ëª»í•œë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤. ì´ë“¤ì€ í…ŒìŠ¤íŠ¸ê°€ í•„ìš” ì—†ëŠ” GPT-4(0~10ì  ì‚¬ì´)ë¥¼ ì‚¬ìš©í•˜ëŠ” LLM ë“±ê¸‰ì„ ì†Œê°œí•©ë‹ˆë‹¤.
![image](https://github.com/SonWY2/paper_caputred_images_repo/assets/36894403/c6f96e01-d77c-47cf-9082-fce895d221a8)



Thereâ€™s a concern that CodeExercises might contain samples that are similar to exercises in HumanEval. The authors propose to remove these samples and retrain phi-1 on this decontaminated set.  
CodeExercisesì— HumanEvalì˜ ì—°ìŠµê³¼ ìœ ì‚¬í•œ ìƒ˜í”Œì´ í¬í•¨ë  ìˆ˜ ìˆë‹¤ëŠ” ìš°ë ¤ê°€ ìˆìŠµë‹ˆë‹¤. ì €ìë“¤ì€ ì´ëŸ¬í•œ ìƒ˜í”Œì„ ì œê±°í•˜ê³  ì˜¤ì—¼ì´ ì œê±°ëœ ì´ ì„¸íŠ¸ì— ëŒ€í•´ phi-1ì„ ì¬í›ˆë ¨í•  ê²ƒì„ ì œì•ˆí•©ë‹ˆë‹¤.

They report no meaningful n-gram overlap between CodeExercises and HumanEval (4 false positives). They then use a combination of embedding and syntax-based distances to find similar code snippets:  
ì´ë“¤ì€ CodeExercisesì™€ HumanEval ì‚¬ì´ì— ì˜ë¯¸ ìˆëŠ” n-ê·¸ë¨ ì¤‘ë³µì´ ì—†ë‹¤ê³  ë³´ê³ í•©ë‹ˆë‹¤(ì˜¤íƒ 4ê±´). ê·¸ëŸ° ë‹¤ìŒ ì„ë² ë”©ê³¼ êµ¬ë¬¸ ê¸°ë°˜ ê±°ë¦¬ì˜ ì¡°í•©ì„ ì‚¬ìš©í•˜ì—¬ ìœ ì‚¬í•œ ì½”ë“œ ì¡°ê°ì„ ì°¾ìŠµë‹ˆë‹¤:


Semantics: They use the L2 distance between embeddings produced by CodeGen  
Syntax: They calculate the (string) edit distance between the abstract syntax trees (ASTs) of two code snippets.  
Despite this data pruning, the authors claim that phi-1 still outperforms StarCoder on HumanEval.  
ì‹œë§¨í‹±: CodeGenì—ì„œ ìƒì„±í•œ ì„ë² ë”© ê°„ì˜ L2 ê±°ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.  
Syntax: ë‘ ì½”ë“œ ìŠ¤ë‹ˆí«ì˜ ì¶”ìƒ êµ¬ë¬¸ íŠ¸ë¦¬(AST) ì‚¬ì´ì˜ (ë¬¸ìì—´) í¸ì§‘ ê±°ë¦¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.  
ì´ëŸ¬í•œ ë°ì´í„° ê°€ì§€ì¹˜ê¸°ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì €ìë“¤ì€ phi-1ì´ ì—¬ì „íˆ HumanEvalì—ì„œ StarCoderë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚˜ë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤.  
êµ¬ë¬¸: ë‘ ì½”ë“œ ìŠ¤ë‹ˆí«ì˜ ì¶”ìƒ êµ¬ë¬¸ íŠ¸ë¦¬(AST) ì‚¬ì´ì˜ (ë¬¸ìì—´) í¸ì§‘ ê±°ë¦¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.  
ì´ëŸ¬í•œ ë°ì´í„° ê°€ì§€ì¹˜ê¸°ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì €ìë“¤ì€ phi-1ì´ ì—¬ì „íˆ HumanEvalì—ì„œ StarCoderë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚˜ë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤.  
