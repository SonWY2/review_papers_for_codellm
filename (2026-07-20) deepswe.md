# DeepSWE: Training a Fully Open-sourced, State-of-the-Art Coding Agent by Scaling RL

## 📄 기본 정보
- **Release Date**: 2025년 7월 2일
- **ArXiv**: 아직 미공개
- **GitHub**: 오픈소스 공개 예정
- **소속**: Agentica team & Together AI

---

[개요]
- Qwen3-32B를 기반으로 순수 Reinforcement Learning만으로 훈련한 오픈소스 코딩 에이전트 DeepSWE-Preview를 소개하며, SWE-Bench-Verified에서 test-time scaling을 통해 59%라는 SOTA 성능을 달성했다.

■ 주요 내용
- **연구 목적**: 순수 RL만으로 long-horizon, multi-step agentic task를 해결하는 코딩 에이전트 개발
- **핵심 아이디어**: Qwen3-32B 위에 rLLM 프레임워크와 GRPO++ 알고리즘을 사용하여 RL 훈련
- **적용 도메인**: Autonomous Software Engineering (SWE) - GitHub 이슈 해결, 코드 구현, 디버깅
- **실험 규모**: 4,500개 실제 SWE 태스크, 6일간 64 H100 GPUs 사용

■ 주요 발견
- **핵심 결과 1**: SWE-Bench-Verified에서 Pass@1 42.2%, test-time scaling으로 59% 달성
- **핵심 결과 2**: 오픈소스 모델 중 최고 성능으로 기존 SOTA(SkyWork + TTS 47%) 대비 12% 향상
- **핵심 결과 3**: Edge case 고려 및 regression test 실행 등 창발적 행동 패턴 발견
- **한계점**: 32K 이상 context length 확장시 성능 향상 제한적 (≤2%)

---

[방법론 및 프레임워크]

■ 전체 프레임워크
- R2E-Gym 환경에서 rLLM 프레임워크를 사용하여 multi-turn agentic task를 위한 RL 훈련 시스템
- LLM 에이전트가 terminal과 filesystem이 포함된 소프트웨어 엔지니어링 환경에서 표준 IDE 도구들(Bash commands, file search, file viewer/editor)을 사용하여 상호작용하는 구조

■ 핵심 구성요소
- R2E-Gym Environment
  - 확장 가능한 고품질 SWE 환경 큐레이션을 위한 Gym 환경
  - Docker 이미지 기반의 실제 소프트웨어 프로젝트 환경 제공
  - **구현 방식**: 4개 도구(Execute Bash, Search, File Editor, Finish/Submit)를 action space로 정의

- rLLM Framework
  - Agentica의 언어 에이전트 post-training 프레임워크
  - multi-turn agent 환경에서 RL 훈련 지원
  - **구현 방식**: ChatML 형식에서 environment observation 마스킹 처리 (system, user, tool 사용은 마스킹)

- GRPO++ Algorithm
  - 기존 GRPO를 개선한 안정적이고 성능이 향상된 RL 알고리즘
  - multi-turn agent 환경에서 정책 최적화
  - **구현 방식**: Clip High, No KL Loss, Leave One Out, Compact Filtering 등 여러 기법 통합
  - GRPO++ 핵심 개선사항
    - Clip High (DAPO): surrogate loss 상한선 증가로 exploration 장려, `torch.clamp(ratio, 1-ε, float('inf'))`. 좋은 action 확률 증가 유도, Exploration 장려 학습 속도 향상
    - No KL Loss (DAPO): KL loss 제거로 trust region 제약 해제. 정책이 데이터로부터 자유롭게 학습. SFT bias 탈피
    - No Reward Standard Deviation: difficulty bias 제거. `advantages = rewards - reward_mean` 쉬운 문제가 낮은 분산을 가져 상대적으로 큰 advantage가 생김. 문제 난이도와 무관하게 공정학 학습 유
    - Length Normalization: max context length로 나누어 length bias 제거. long sequence가 총 loss가 커져 불리한 왜국 발생. `normalized_loss = sum(losses) / max_context_length` 길이와 무관한 품질 기준 학습하고 충분한 사고 과정 장려(multi-turn 환경 고려)
    - Leave One Out (LOOP/RLOO): advantage estimation에서 variance 감소. 각 샘플의 baseline 계산시 자기 자신 제외. 분산 감소, 편향 제거.
    - Compact Filtering: max context/timeout/max steps 도달한 trajectory 마스킹. DeepSWE 결과 response length는 감소하고 environment step은 증가함
    - No Entropy Loss: 불안정성 제거. Base model entropy가 0.3-1 범위일 때 조건부로 entropy loss 제거
   
- Test-Time Scaling (TTS) System
  - 추론 시 다수의 trajectory를 생성하여 최적해를 선택하는 시스템
  - 단일 추론보다 높은 성능 달성을 위한 scaling 메커니즘
  - 구현 방식: Execution-free, Execution-based, Hybrid verifier를 통한 trajectory 선택

- Verifier
  - 생성된 trajectory들 중 올바른 해답을 선택하는 판별 모델
  - 다중 candidate 중 최고 품질 trajectory 식별
  - Hybrid verifier system으로 execution-free와 execution-based 방법의 장점 결합
  - 구현 방식: DeepSWE-Verifier (execution-free) + test coverage verifier (execution-based)
  - (테이블) Deepswe-verifier
    - Correct/incorrect patch pairs에 대해 2 epoch 학습
    - 코드 품질을 정적 분석으로 평가
  - Execution-based Verifier
    - test case 실행 검증
  - Hybrid Verifier(최종방식)
    - Execution-free + Execution-based scores 통합
    - 59% 달성 (Execution-free 단독 시 ~47%)
    


■ 기술적 혁신점
- SFT 없이 순수 RL만으로 코딩 에이전트 훈련 성공
- Compact Filtering을 통한 reward collapse 방지 및 multi-step reasoning 장려
- Single-step GRPO를 multi-turn agent 환경으로 확장하는 안정적 방법론 제시

---

## 🧪 실험 세팅

### 모델
- **모델**: Qwen/Qwen3-32B (base model)
- Thinking mode 지원, 16K-128K context length

### 데이터셋
- **주요 벤치마크**: 
  - **R2E-Gym subset**: 4,500개 SWE 문제, SWE-Bench-Verified와 동일 repository 제외
  - **SWE-Bench-Verified**: 500개 검증된 실제 GitHub 이슈 (평가용)
- **전처리**: Docker 이미지 매핑, data contamination 방지를 위한 repository 필터링
- **분할 방식**: 훈련용 4.5K / 평가용 500개 문제

### 평가 지표
- **주요 메트릭**: Pass@K (k개 샘플 중 하나라도 모든 테스트 통과 확률)
- **추가 메트릭**: Pass@1, Pass@16, test-time scaling 성능
- **베이스라인**: SWE-Agent-LM, OpenHands-LM, Skywork-SWE, Devstral-Small 등

### 실험 설정 상세
**RL Training**
- **목적**: RL을 통한 코딩 에이전트 훈련
- **설정**: 64 H100 GPUs, 6일간 훈련, 512 parallel containers per iteration
- **파라미터**: Batch size 64, 8 passes, 5분 timeout limit, sparse ORM reward

**Test-Time Scaling**
- **목적**: 다양한 TTS 전략의 효과성 검증
- **설정**: Execution-free, Execution-based, Hybrid verifier 비교
- **파라미터**: K=8, K=16 rollouts, 64K context length

---

[실험 결과]

- 주요 성능 결과:
  SWE-Bench-Verified에서 모델 크기 대비 성능 비교. DeepSWE-Preview가 test-time scaling으로 59% 달성하여 모든 오픈소스 에이전트를 크게 앞섬
  <img width="3131" height="2194" alt="image" src="https://github.com/user-attachments/assets/3b1d8bf0-fac9-491c-baa4-f01d21a567d4" />

- 정량적 결과
  - **Pass@1**: DeepSWE-Preview 42.2% vs 기존 최고 SWE-Agent-LM 40.2%
  - **Pass@16**: 71.0% 달성
  - **Test-time scaling**: 59.0% (Hybrid Best@16), 57.9% (Hybrid Best@8)

- 정성적 분석
  - **관찰된 패턴**: RL 훈련 200 스텝만으로 23%→42% (+20%) 향상
  - **예상과 다른 결과**: Context length 확장의 효과가 제한적 (32K 이후 ≤2% 향상)
  - **흥미로운 발견**: Edge case 고려 및 regression test 자동 실행 등 창발적 행동

### 세부 실험 결과

**RL Training Progress**
> **Figure 2**: SWE-Bench-Hard validation score에서 200 스텝 RL 훈련으로 23%→42% 성능 향상

- **핵심 발견**: 순수 RL만으로도 빠른 성능 향상 가능
- **분석**: Sparse reward (0/1)에도 불구하고 안정적 학습 진행

**Test-Time Scaling Analysis**
> **Figure 10**: 다양한 TTS 전략별 SWE-Bench-Verified 성능. Hybrid TTS가 59%로 최고 성능

- **핵심 발견**: Hybrid approach가 individual verifier들보다 현저히 우수
- **분석**: K=8에서도 대부분의 TTS 성능 향상 달성 가능

### Ablation Study
**GRPO vs GRPO++**
> **Figure 5**: FrozenLake에서 GRPO++ 알고리즘이 기존 GRPO보다 빠른 학습 속도 보임

- **결과**: GRPO++가 안정성과 성능 모두에서 우수
- **영향**: Clip High, No KL Loss, Leave One Out이 주요 기여 요소
- **인사이트**: Multi-turn 환경에서 기존 GRPO 한계 극복

**Compact Filtering**
> **Figure 6**: Qwen3-14B에서 compact filtering 적용 여부에 따른 성능 차이

- **결과**: Compact filtering이 reward collapse 방지 및 성능 향상
- **영향**: Average response length 감소, environment steps 증가
- **인사이트**: Step별 적응적 thinking token 할당 학습

### 모델별/조건별 성능 차이
- **모델 크기에 따른 차이**: 32B가 14B 대비 상당한 성능 향상
- **데이터 규모에 따른 차이**: 4.5K 문제로도 충분한 성능 달성
- **난이도에 따른 차이**: SWE-Bench-Verified(고난도)에서도 안정적 성능

---

[결론]
- **Pure RL Training**: SFT 없이 순수 RL만으로 SOTA 코딩 에이전트 훈련 성공
- **GRPO++ Algorithm**: Multi-turn agent 환경을 위한 안정적 RL 알고리즘 개발
- **Compact Filtering**: Reward collapse 방지 및 adaptive thinking 유도하는 새로운 필터링 기법
- 한계점 및 향후 연구
  - **현재 방법의 한계**: 특정 데이터셋에 대한 의존성
  - **해결되지 않은 문제**: 최적의 데이터 큐레이션 방법, 더 큰 모델에서의 확장성

---
