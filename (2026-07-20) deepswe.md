# DeepSWE: Training a Fully Open-sourced, State-of-the-Art Coding Agent by Scaling RL

## ğŸ“„ ê¸°ë³¸ ì •ë³´
- **Release Date**: 2025ë…„ 7ì›” 2ì¼
- **ArXiv**: ì•„ì§ ë¯¸ê³µê°œ
- **GitHub**: ì˜¤í”ˆì†ŒìŠ¤ ê³µê°œ ì˜ˆì •
- **ì†Œì†**: Agentica team & Together AI

---

[ê°œìš”]
- Qwen3-32Bë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìˆœìˆ˜ Reinforcement Learningë§Œìœ¼ë¡œ í›ˆë ¨í•œ ì˜¤í”ˆì†ŒìŠ¤ ì½”ë”© ì—ì´ì „íŠ¸ DeepSWE-Previewë¥¼ ì†Œê°œí•˜ë©°, SWE-Bench-Verifiedì—ì„œ test-time scalingì„ í†µí•´ 59%ë¼ëŠ” SOTA ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆë‹¤.

â–  ì£¼ìš” ë‚´ìš©
- **ì—°êµ¬ ëª©ì **: ìˆœìˆ˜ RLë§Œìœ¼ë¡œ long-horizon, multi-step agentic taskë¥¼ í•´ê²°í•˜ëŠ” ì½”ë”© ì—ì´ì „íŠ¸ ê°œë°œ
- **í•µì‹¬ ì•„ì´ë””ì–´**: Qwen3-32B ìœ„ì— rLLM í”„ë ˆì„ì›Œí¬ì™€ GRPO++ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ RL í›ˆë ¨
- **ì ìš© ë„ë©”ì¸**: Autonomous Software Engineering (SWE) - GitHub ì´ìŠˆ í•´ê²°, ì½”ë“œ êµ¬í˜„, ë””ë²„ê¹…
- **ì‹¤í—˜ ê·œëª¨**: 4,500ê°œ ì‹¤ì œ SWE íƒœìŠ¤í¬, 6ì¼ê°„ 64 H100 GPUs ì‚¬ìš©

â–  ì£¼ìš” ë°œê²¬
- **í•µì‹¬ ê²°ê³¼ 1**: SWE-Bench-Verifiedì—ì„œ Pass@1 42.2%, test-time scalingìœ¼ë¡œ 59% ë‹¬ì„±
- **í•µì‹¬ ê²°ê³¼ 2**: ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ ì¤‘ ìµœê³  ì„±ëŠ¥ìœ¼ë¡œ ê¸°ì¡´ SOTA(SkyWork + TTS 47%) ëŒ€ë¹„ 12% í–¥ìƒ
- **í•µì‹¬ ê²°ê³¼ 3**: Edge case ê³ ë ¤ ë° regression test ì‹¤í–‰ ë“± ì°½ë°œì  í–‰ë™ íŒ¨í„´ ë°œê²¬
- **í•œê³„ì **: 32K ì´ìƒ context length í™•ì¥ì‹œ ì„±ëŠ¥ í–¥ìƒ ì œí•œì  (â‰¤2%)

---

[ë°©ë²•ë¡  ë° í”„ë ˆì„ì›Œí¬]

â–  ì „ì²´ í”„ë ˆì„ì›Œí¬
- R2E-Gym í™˜ê²½ì—ì„œ rLLM í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ multi-turn agentic taskë¥¼ ìœ„í•œ RL í›ˆë ¨ ì‹œìŠ¤í…œ
- LLM ì—ì´ì „íŠ¸ê°€ terminalê³¼ filesystemì´ í¬í•¨ëœ ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ë§ í™˜ê²½ì—ì„œ í‘œì¤€ IDE ë„êµ¬ë“¤(Bash commands, file search, file viewer/editor)ì„ ì‚¬ìš©í•˜ì—¬ ìƒí˜¸ì‘ìš©í•˜ëŠ” êµ¬ì¡°

â–  í•µì‹¬ êµ¬ì„±ìš”ì†Œ
- R2E-Gym Environment
  - í™•ì¥ ê°€ëŠ¥í•œ ê³ í’ˆì§ˆ SWE í™˜ê²½ íë ˆì´ì…˜ì„ ìœ„í•œ Gym í™˜ê²½
  - Docker ì´ë¯¸ì§€ ê¸°ë°˜ì˜ ì‹¤ì œ ì†Œí”„íŠ¸ì›¨ì–´ í”„ë¡œì íŠ¸ í™˜ê²½ ì œê³µ
  - **êµ¬í˜„ ë°©ì‹**: 4ê°œ ë„êµ¬(Execute Bash, Search, File Editor, Finish/Submit)ë¥¼ action spaceë¡œ ì •ì˜

- rLLM Framework
  - Agenticaì˜ ì–¸ì–´ ì—ì´ì „íŠ¸ post-training í”„ë ˆì„ì›Œí¬
  - multi-turn agent í™˜ê²½ì—ì„œ RL í›ˆë ¨ ì§€ì›
  - **êµ¬í˜„ ë°©ì‹**: ChatML í˜•ì‹ì—ì„œ environment observation ë§ˆìŠ¤í‚¹ ì²˜ë¦¬ (system, user, tool ì‚¬ìš©ì€ ë§ˆìŠ¤í‚¹)

- GRPO++ Algorithm
  - ê¸°ì¡´ GRPOë¥¼ ê°œì„ í•œ ì•ˆì •ì ì´ê³  ì„±ëŠ¥ì´ í–¥ìƒëœ RL ì•Œê³ ë¦¬ì¦˜
  - multi-turn agent í™˜ê²½ì—ì„œ ì •ì±… ìµœì í™”
  - **êµ¬í˜„ ë°©ì‹**: Clip High, No KL Loss, Leave One Out, Compact Filtering ë“± ì—¬ëŸ¬ ê¸°ë²• í†µí•©
  - GRPO++ í•µì‹¬ ê°œì„ ì‚¬í•­
    - Clip High (DAPO): surrogate loss ìƒí•œì„  ì¦ê°€ë¡œ exploration ì¥ë ¤, `torch.clamp(ratio, 1-Îµ, float('inf'))`. ì¢‹ì€ action í™•ë¥  ì¦ê°€ ìœ ë„, Exploration ì¥ë ¤ í•™ìŠµ ì†ë„ í–¥ìƒ
    - No KL Loss (DAPO): KL loss ì œê±°ë¡œ trust region ì œì•½ í•´ì œ. ì •ì±…ì´ ë°ì´í„°ë¡œë¶€í„° ììœ ë¡­ê²Œ í•™ìŠµ. SFT bias íƒˆí”¼
    - No Reward Standard Deviation: difficulty bias ì œê±°. `advantages = rewards - reward_mean` ì‰¬ìš´ ë¬¸ì œê°€ ë‚®ì€ ë¶„ì‚°ì„ ê°€ì ¸ ìƒëŒ€ì ìœ¼ë¡œ í° advantageê°€ ìƒê¹€. ë¬¸ì œ ë‚œì´ë„ì™€ ë¬´ê´€í•˜ê²Œ ê³µì •í•™ í•™ìŠµ ìœ 
    - Length Normalization: max context lengthë¡œ ë‚˜ëˆ„ì–´ length bias ì œê±°. long sequenceê°€ ì´ lossê°€ ì»¤ì ¸ ë¶ˆë¦¬í•œ ì™œêµ­ ë°œìƒ. `normalized_loss = sum(losses) / max_context_length` ê¸¸ì´ì™€ ë¬´ê´€í•œ í’ˆì§ˆ ê¸°ì¤€ í•™ìŠµí•˜ê³  ì¶©ë¶„í•œ ì‚¬ê³  ê³¼ì • ì¥ë ¤(multi-turn í™˜ê²½ ê³ ë ¤)
    - Leave One Out (LOOP/RLOO): advantage estimationì—ì„œ variance ê°ì†Œ. ê° ìƒ˜í”Œì˜ baseline ê³„ì‚°ì‹œ ìê¸° ìì‹  ì œì™¸. ë¶„ì‚° ê°ì†Œ, í¸í–¥ ì œê±°.
    - Compact Filtering: max context/timeout/max steps ë„ë‹¬í•œ trajectory ë§ˆìŠ¤í‚¹. DeepSWE ê²°ê³¼ response lengthëŠ” ê°ì†Œí•˜ê³  environment stepì€ ì¦ê°€í•¨
    - No Entropy Loss: ë¶ˆì•ˆì •ì„± ì œê±°. Base model entropyê°€ 0.3-1 ë²”ìœ„ì¼ ë•Œ ì¡°ê±´ë¶€ë¡œ entropy loss ì œê±°
   
- Test-Time Scaling (TTS) System
  - ì¶”ë¡  ì‹œ ë‹¤ìˆ˜ì˜ trajectoryë¥¼ ìƒì„±í•˜ì—¬ ìµœì í•´ë¥¼ ì„ íƒí•˜ëŠ” ì‹œìŠ¤í…œ
  - ë‹¨ì¼ ì¶”ë¡ ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±ì„ ìœ„í•œ scaling ë©”ì»¤ë‹ˆì¦˜
  - êµ¬í˜„ ë°©ì‹: Execution-free, Execution-based, Hybrid verifierë¥¼ í†µí•œ trajectory ì„ íƒ

- Verifier
  - ìƒì„±ëœ trajectoryë“¤ ì¤‘ ì˜¬ë°”ë¥¸ í•´ë‹µì„ ì„ íƒí•˜ëŠ” íŒë³„ ëª¨ë¸
  - ë‹¤ì¤‘ candidate ì¤‘ ìµœê³  í’ˆì§ˆ trajectory ì‹ë³„
  - Hybrid verifier systemìœ¼ë¡œ execution-freeì™€ execution-based ë°©ë²•ì˜ ì¥ì  ê²°í•©
  - êµ¬í˜„ ë°©ì‹: DeepSWE-Verifier (execution-free) + test coverage verifier (execution-based)
  - (í…Œì´ë¸”) Deepswe-verifier
    - Correct/incorrect patch pairsì— ëŒ€í•´ 2 epoch í•™ìŠµ
    - ì½”ë“œ í’ˆì§ˆì„ ì •ì  ë¶„ì„ìœ¼ë¡œ í‰ê°€
  - Execution-based Verifier
    - test case ì‹¤í–‰ ê²€ì¦
  - Hybrid Verifier(ìµœì¢…ë°©ì‹)
    - Execution-free + Execution-based scores í†µí•©
    - 59% ë‹¬ì„± (Execution-free ë‹¨ë… ì‹œ ~47%)
    


â–  ê¸°ìˆ ì  í˜ì‹ ì 
- SFT ì—†ì´ ìˆœìˆ˜ RLë§Œìœ¼ë¡œ ì½”ë”© ì—ì´ì „íŠ¸ í›ˆë ¨ ì„±ê³µ
- Compact Filteringì„ í†µí•œ reward collapse ë°©ì§€ ë° multi-step reasoning ì¥ë ¤
- Single-step GRPOë¥¼ multi-turn agent í™˜ê²½ìœ¼ë¡œ í™•ì¥í•˜ëŠ” ì•ˆì •ì  ë°©ë²•ë¡  ì œì‹œ

---

## ğŸ§ª ì‹¤í—˜ ì„¸íŒ…

### ëª¨ë¸
- **ëª¨ë¸**: Qwen/Qwen3-32B (base model)
- Thinking mode ì§€ì›, 16K-128K context length

### ë°ì´í„°ì…‹
- **ì£¼ìš” ë²¤ì¹˜ë§ˆí¬**: 
  - **R2E-Gym subset**: 4,500ê°œ SWE ë¬¸ì œ, SWE-Bench-Verifiedì™€ ë™ì¼ repository ì œì™¸
  - **SWE-Bench-Verified**: 500ê°œ ê²€ì¦ëœ ì‹¤ì œ GitHub ì´ìŠˆ (í‰ê°€ìš©)
- **ì „ì²˜ë¦¬**: Docker ì´ë¯¸ì§€ ë§¤í•‘, data contamination ë°©ì§€ë¥¼ ìœ„í•œ repository í•„í„°ë§
- **ë¶„í•  ë°©ì‹**: í›ˆë ¨ìš© 4.5K / í‰ê°€ìš© 500ê°œ ë¬¸ì œ

### í‰ê°€ ì§€í‘œ
- **ì£¼ìš” ë©”íŠ¸ë¦­**: Pass@K (kê°œ ìƒ˜í”Œ ì¤‘ í•˜ë‚˜ë¼ë„ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼ í™•ë¥ )
- **ì¶”ê°€ ë©”íŠ¸ë¦­**: Pass@1, Pass@16, test-time scaling ì„±ëŠ¥
- **ë² ì´ìŠ¤ë¼ì¸**: SWE-Agent-LM, OpenHands-LM, Skywork-SWE, Devstral-Small ë“±

### ì‹¤í—˜ ì„¤ì • ìƒì„¸
**RL Training**
- **ëª©ì **: RLì„ í†µí•œ ì½”ë”© ì—ì´ì „íŠ¸ í›ˆë ¨
- **ì„¤ì •**: 64 H100 GPUs, 6ì¼ê°„ í›ˆë ¨, 512 parallel containers per iteration
- **íŒŒë¼ë¯¸í„°**: Batch size 64, 8 passes, 5ë¶„ timeout limit, sparse ORM reward

**Test-Time Scaling**
- **ëª©ì **: ë‹¤ì–‘í•œ TTS ì „ëµì˜ íš¨ê³¼ì„± ê²€ì¦
- **ì„¤ì •**: Execution-free, Execution-based, Hybrid verifier ë¹„êµ
- **íŒŒë¼ë¯¸í„°**: K=8, K=16 rollouts, 64K context length

---

[ì‹¤í—˜ ê²°ê³¼]

- ì£¼ìš” ì„±ëŠ¥ ê²°ê³¼:
  SWE-Bench-Verifiedì—ì„œ ëª¨ë¸ í¬ê¸° ëŒ€ë¹„ ì„±ëŠ¥ ë¹„êµ. DeepSWE-Previewê°€ test-time scalingìœ¼ë¡œ 59% ë‹¬ì„±í•˜ì—¬ ëª¨ë“  ì˜¤í”ˆì†ŒìŠ¤ ì—ì´ì „íŠ¸ë¥¼ í¬ê²Œ ì•ì„¬
  <img width="3131" height="2194" alt="image" src="https://github.com/user-attachments/assets/3b1d8bf0-fac9-491c-baa4-f01d21a567d4" />

- ì •ëŸ‰ì  ê²°ê³¼
  - **Pass@1**: DeepSWE-Preview 42.2% vs ê¸°ì¡´ ìµœê³  SWE-Agent-LM 40.2%
  - **Pass@16**: 71.0% ë‹¬ì„±
  - **Test-time scaling**: 59.0% (Hybrid Best@16), 57.9% (Hybrid Best@8)

- ì •ì„±ì  ë¶„ì„
  - **ê´€ì°°ëœ íŒ¨í„´**: RL í›ˆë ¨ 200 ìŠ¤í…ë§Œìœ¼ë¡œ 23%â†’42% (+20%) í–¥ìƒ
  - **ì˜ˆìƒê³¼ ë‹¤ë¥¸ ê²°ê³¼**: Context length í™•ì¥ì˜ íš¨ê³¼ê°€ ì œí•œì  (32K ì´í›„ â‰¤2% í–¥ìƒ)
  - **í¥ë¯¸ë¡œìš´ ë°œê²¬**: Edge case ê³ ë ¤ ë° regression test ìë™ ì‹¤í–‰ ë“± ì°½ë°œì  í–‰ë™

### ì„¸ë¶€ ì‹¤í—˜ ê²°ê³¼

**RL Training Progress**
> **Figure 2**: SWE-Bench-Hard validation scoreì—ì„œ 200 ìŠ¤í… RL í›ˆë ¨ìœ¼ë¡œ 23%â†’42% ì„±ëŠ¥ í–¥ìƒ

- **í•µì‹¬ ë°œê²¬**: ìˆœìˆ˜ RLë§Œìœ¼ë¡œë„ ë¹ ë¥¸ ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥
- **ë¶„ì„**: Sparse reward (0/1)ì—ë„ ë¶ˆêµ¬í•˜ê³  ì•ˆì •ì  í•™ìŠµ ì§„í–‰

**Test-Time Scaling Analysis**
> **Figure 10**: ë‹¤ì–‘í•œ TTS ì „ëµë³„ SWE-Bench-Verified ì„±ëŠ¥. Hybrid TTSê°€ 59%ë¡œ ìµœê³  ì„±ëŠ¥

- **í•µì‹¬ ë°œê²¬**: Hybrid approachê°€ individual verifierë“¤ë³´ë‹¤ í˜„ì €íˆ ìš°ìˆ˜
- **ë¶„ì„**: K=8ì—ì„œë„ ëŒ€ë¶€ë¶„ì˜ TTS ì„±ëŠ¥ í–¥ìƒ ë‹¬ì„± ê°€ëŠ¥

### Ablation Study
**GRPO vs GRPO++**
> **Figure 5**: FrozenLakeì—ì„œ GRPO++ ì•Œê³ ë¦¬ì¦˜ì´ ê¸°ì¡´ GRPOë³´ë‹¤ ë¹ ë¥¸ í•™ìŠµ ì†ë„ ë³´ì„

- **ê²°ê³¼**: GRPO++ê°€ ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ ëª¨ë‘ì—ì„œ ìš°ìˆ˜
- **ì˜í–¥**: Clip High, No KL Loss, Leave One Outì´ ì£¼ìš” ê¸°ì—¬ ìš”ì†Œ
- **ì¸ì‚¬ì´íŠ¸**: Multi-turn í™˜ê²½ì—ì„œ ê¸°ì¡´ GRPO í•œê³„ ê·¹ë³µ

**Compact Filtering**
> **Figure 6**: Qwen3-14Bì—ì„œ compact filtering ì ìš© ì—¬ë¶€ì— ë”°ë¥¸ ì„±ëŠ¥ ì°¨ì´

- **ê²°ê³¼**: Compact filteringì´ reward collapse ë°©ì§€ ë° ì„±ëŠ¥ í–¥ìƒ
- **ì˜í–¥**: Average response length ê°ì†Œ, environment steps ì¦ê°€
- **ì¸ì‚¬ì´íŠ¸**: Stepë³„ ì ì‘ì  thinking token í• ë‹¹ í•™ìŠµ

### ëª¨ë¸ë³„/ì¡°ê±´ë³„ ì„±ëŠ¥ ì°¨ì´
- **ëª¨ë¸ í¬ê¸°ì— ë”°ë¥¸ ì°¨ì´**: 32Bê°€ 14B ëŒ€ë¹„ ìƒë‹¹í•œ ì„±ëŠ¥ í–¥ìƒ
- **ë°ì´í„° ê·œëª¨ì— ë”°ë¥¸ ì°¨ì´**: 4.5K ë¬¸ì œë¡œë„ ì¶©ë¶„í•œ ì„±ëŠ¥ ë‹¬ì„±
- **ë‚œì´ë„ì— ë”°ë¥¸ ì°¨ì´**: SWE-Bench-Verified(ê³ ë‚œë„)ì—ì„œë„ ì•ˆì •ì  ì„±ëŠ¥

---

[ê²°ë¡ ]
- **Pure RL Training**: SFT ì—†ì´ ìˆœìˆ˜ RLë§Œìœ¼ë¡œ SOTA ì½”ë”© ì—ì´ì „íŠ¸ í›ˆë ¨ ì„±ê³µ
- **GRPO++ Algorithm**: Multi-turn agent í™˜ê²½ì„ ìœ„í•œ ì•ˆì •ì  RL ì•Œê³ ë¦¬ì¦˜ ê°œë°œ
- **Compact Filtering**: Reward collapse ë°©ì§€ ë° adaptive thinking ìœ ë„í•˜ëŠ” ìƒˆë¡œìš´ í•„í„°ë§ ê¸°ë²•
- í•œê³„ì  ë° í–¥í›„ ì—°êµ¬
  - **í˜„ì¬ ë°©ë²•ì˜ í•œê³„**: íŠ¹ì • ë°ì´í„°ì…‹ì— ëŒ€í•œ ì˜ì¡´ì„±
  - **í•´ê²°ë˜ì§€ ì•Šì€ ë¬¸ì œ**: ìµœì ì˜ ë°ì´í„° íë ˆì´ì…˜ ë°©ë²•, ë” í° ëª¨ë¸ì—ì„œì˜ í™•ì¥ì„±

---
